{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_PATH = os.getcwd()\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, \"models\", \"pretrained\")\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = MODEL_PATH\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from transformers import MAMConfig\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\n",
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "logger = logging.get_logger(\"transformers\")\n",
    "logger.info(\"INFO\")\n",
    "logger.warning(\"WARNING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at h:\\cs544\\project\\models\\pretrained\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\vocab.json\n",
      "loading file merges.txt from cache at h:\\cs544\\project\\models\\pretrained\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at h:\\cs544\\project\\models\\pretrained\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at h:\\cs544\\project\\models\\pretrained\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at h:\\cs544\\project\\models\\pretrained\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2Model.\n",
      "\n",
      "All the weights of GPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    \"gpt2\", \n",
    "    padding_side=\"right\"\n",
    ")\n",
    "baseGPT2 = GPT2Model.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = os.path.join(PROJECT_PATH, \"results\")\n",
    "LOG_PATH = os.path.join(PROJECT_PATH, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_NAME = \"mam_adpater\"\n",
    "ADAPTER_CONFIG = MAMConfig()\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "SEQUENCE_LENGTH = 128\n",
    "SEQUENCE_EMEBDDING_SIZE = 512\n",
    "CNN_WINDOW_SIZE = 5\n",
    "FF_HIDDEN_SIZE = 2048\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNGPT2(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        sequence_length,        #   Length of input sequence\n",
    "        adapter_name,           #   Name of the adapter\n",
    "        adapter_config,         #   Adapter config\n",
    "        num_classes,            #   Number of classes\n",
    "        cnn_output_channels,    #   Dimension of target sequence embedding\n",
    "        cnn_kernel_size,        #   Window size of 1d CNN\n",
    "        ff_hidden_size,         #   Hidden size of FeedForward layer\n",
    "        loss_fn=None,           #   Loss function\n",
    "    ):\n",
    "        super(CNNGPT2, self).__init__()\n",
    "\n",
    "        #   Pre-trained GPT-2 encoder\n",
    "        self.gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "        #   Freeze GPT-2 pre-trained parameters\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        #   Adapter on GPT-2\n",
    "        self.gpt2.add_adapter(adapter_name, config=adapter_config)\n",
    "        self.gpt2.set_active_adapters(adapter_name)\n",
    "\n",
    "        #   CNN layer\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels=self.gpt2.config.hidden_size * 2,\n",
    "            out_channels=cnn_output_channels,\n",
    "            kernel_size=cnn_kernel_size,\n",
    "            padding=np.floor(cnn_kernel_size / 2).astype(int)\n",
    "        )\n",
    "\n",
    "        #   Pooling layer (Max Pooling)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=sequence_length)\n",
    "\n",
    "        #   FeedForward layers\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(cnn_output_channels, ff_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "        #   Batch Normalization Layers\n",
    "        self.word_embedding_bn = nn.BatchNorm1d(self.gpt2.config.hidden_size)\n",
    "        self.encode_bn = nn.BatchNorm1d(self.gpt2.config.hidden_size)\n",
    "        self.pooling_bn = nn.BatchNorm1d(cnn_output_channels)\n",
    "        \n",
    "        self.loss_fn = loss_fn\n",
    "        if self.loss_fn is None:\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, batched_sequence, attention_mask, labels=None):\n",
    "        #   Denote B := batch size, L := sequence length, H := hidden size, H' := cnn_output_channels\n",
    "\n",
    "        #   Dimension of batched_sequence: (B, L)\n",
    "        #   Dimension of attention_mask: (B, L)\n",
    "\n",
    "        #   Use GPT-2 to create basic representations of sequences\n",
    "        outputs = self.gpt2(\n",
    "            batched_sequence, \n",
    "            attention_mask=attention_mask, \n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        #   The outputs contains the hidden states from each layer of the GPT-2 encoder\n",
    "        #   0th is the word embeddings, and the last is the last hidden state\n",
    "\n",
    "        #   Extract word embeddings and last hidden states\n",
    "        word_embeddings = outputs.hidden_states[0]\n",
    "        #   Dimension of word_embeddings: (B, L, H)\n",
    "        encoder_hidden_state = outputs.last_hidden_state\n",
    "        #   Dimension of encoder_hidden_state: (B, L, H)\n",
    "\n",
    "        #   Batch Normalization\n",
    "        word_embeddings = self.word_embedding_bn(word_embeddings.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        encoder_hidden_state = self.encode_bn(encoder_hidden_state.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        #   Concatenate word embeddings and last hidden states\n",
    "        concatenated_embeddings = torch.cat((word_embeddings, encoder_hidden_state), dim=-1)\n",
    "        #   Dimension of concatenated_embeddings: (B, L, H*2)\n",
    "\n",
    "        #   Apply attention mask to the concatenated sequence representation\n",
    "        #   The attention mask is expanded to dimension (B, L, H*2), \n",
    "        #   and then multiplied element-wise with the concatenated sequence representation. \n",
    "        #   This is zeros out the values at the padded positions.\n",
    "        concatenated_embeddings = concatenated_embeddings * \\\n",
    "            attention_mask.unsqueeze(-1).expand(concatenated_embeddings.shape)\n",
    "\n",
    "        #   Apply CNN\n",
    "        cnn_out = self.cnn(concatenated_embeddings.permute(0, 2, 1))\n",
    "        #   Dimension of cnn_out: (B, H', L)\n",
    "\n",
    "        # Pooling\n",
    "        pooled = self.max_pool(cnn_out)\n",
    "        #   Dimension of pooled: (B, H', 1)\n",
    "\n",
    "        #   Batch Normalization on pooled sequence representation\n",
    "        #   This is the final sequence embedding\n",
    "        pooled = self.pooling_bn(pooled.squeeze(-1))\n",
    "        #   Dimension of pooled: (B, H')\n",
    "\n",
    "        #   Apply FeedForward\n",
    "        logits = self.ff(pooled)\n",
    "        #   Dimension of logits: (B, num_classes)\n",
    "\n",
    "        #   Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            #   The loss function is CrossEntropyLoss or otherwise specified at initialization\n",
    "            assert self.loss_fn is not None\n",
    "\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        #   Denote B := batch size, L := sequence length\n",
    "\n",
    "        #   Extract sequences from batch\n",
    "        sequences = [item[\"seq\"] for item in batch]\n",
    "        #   Dimension of sequences: (B, L)\n",
    "\n",
    "        #   Extract labels from batch\n",
    "        labels = [item[\"label\"] for item in batch]\n",
    "        #   Dimension of labels: (B,)\n",
    "\n",
    "        #   Tokenize sequences\n",
    "        tokenized_sequences = self.tokenizer(\n",
    "            sequences, \n",
    "            padding=\"max_length\", \n",
    "            max_length=SEQUENCE_LENGTH,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        #   Dimension of tokenized_sequences[\"input_ids\"]: (B, L)\n",
    "        #   Dimension of tokenized_sequences[\"attention_mask\"]: (B, L)\n",
    "\n",
    "        #   Convert labels to tensor\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        return {\"batched_sequence\": tokenized_sequences, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        #   Denote B := batch size, L := sequence length\n",
    "\n",
    "        #   Extract batched sequence and attention mask\n",
    "        batched_sequence = inputs[\"batched_sequence\"][\"input_ids\"]\n",
    "        attention_mask = inputs[\"batched_sequence\"][\"attention_mask\"]\n",
    "        #   Dimension of batched_sequence: (B, L)\n",
    "        #   Dimension of attention_mask: (B, L)\n",
    "\n",
    "        #   Extract labels\n",
    "        labels = inputs[\"labels\"]\n",
    "        #   Dimension of labels: (B,)\n",
    "\n",
    "        #   Apply model to batch of inputs\n",
    "        outputs = model(batched_sequence, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        #   Dimension of logits: (B, num_classes)\n",
    "\n",
    "        #   Compute loss with Cross Entropy Loss\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, items: list):\n",
    "        self.items = items\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"seq\": self.items[index][\"seq\"], \n",
    "            \"label\": self.items[index][\"label\"]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = [\n",
    "    {\"seq\": \"Hello world!\", \"label\": 0},\n",
    "    {\"seq\": \"This is a random sequence\", \"label\": 1},\n",
    "    {\"seq\": \"Just another example\", \"label\": 2},\n",
    "    {\"seq\": \"Sample number four\", \"label\": 0},\n",
    "    {\"seq\": \"Trying different things\", \"label\": 1},\n",
    "    {\"seq\": \"Have a great day!\", \"label\": 2},\n",
    "    {\"seq\": \"Keep up the good work\", \"label\": 0},\n",
    "    {\"seq\": \"Today is a sunny day\", \"label\": 1},\n",
    "    {\"seq\": \"Learning new things\", \"label\": 2},\n",
    "    {\"seq\": \"Stay positive and motivated\", \"label\": 0},\n",
    "]\n",
    "\n",
    "eval_samples = [\n",
    "    {\"seq\": \"Evaluate this sample\", \"label\": 1},\n",
    "    {\"seq\": \"Checking for errors\", \"label\": 2},\n",
    "    {\"seq\": \"A simple test case\", \"label\": 0},\n",
    "    {\"seq\": \"This is interesting\", \"label\": 1},\n",
    "    {\"seq\": \"Randomly selected label\", \"label\": 2},\n",
    "    {\"seq\": \"Keep moving forward\", \"label\": 0},\n",
    "    {\"seq\": \"One more example\", \"label\": 1},\n",
    "    {\"seq\": \"Happy to help\", \"label\": 2},\n",
    "    {\"seq\": \"Let's see how it works\", \"label\": 0},\n",
    "    {\"seq\": \"The final evaluation sample\", \"label\": 1},\n",
    "]\n",
    "\n",
    "TrainSet = CustomDataset(train_samples)\n",
    "EvalSet = CustomDataset(eval_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyCollator = CustomDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyCollator.tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at h:\\cs544\\project\\models\\pretrained\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at h:\\cs544\\project\\models\\pretrained\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2Model.\n",
      "\n",
      "All the weights of GPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2Model for predictions without further training.\n",
      "Adding adapter 'mam_adpater'.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "custom_model = CNNGPT2(\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    adapter_name=ADAPTER_NAME,\n",
    "    adapter_config=ADAPTER_CONFIG,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    cnn_output_channels=SEQUENCE_EMEBDDING_SIZE,\n",
    "    cnn_kernel_size=CNN_WINDOW_SIZE,\n",
    "    ff_hidden_size=FF_HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=RESULTS_PATH,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=LOG_PATH,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=custom_model,\n",
    "    args=training_args,\n",
    "    data_collator=MyCollator,\n",
    "    train_dataset=TrainSet,\n",
    "    eval_dataset=EvalSet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = trainer.get_train_dataloader()\n",
    "eval_loader = trainer.get_eval_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\envs\\transformer\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 27487523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738f8aff5a8342deae7ada547c39146c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2098, 'learning_rate': 4.5e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1961, 'learning_rate': 4e-05, 'epoch': 1.0}\n",
      "{'loss': 0.8524, 'learning_rate': 3e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6093, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.519, 'learning_rate': 1e-05, 'epoch': 4.0}\n",
      "{'loss': 0.4656, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 2.8687, 'train_samples_per_second': 17.43, 'train_steps_per_second': 3.486, 'train_loss': 0.7298534274101257, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2f0d3aad614024803ac943109a86d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "eval_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.093057632446289,\n",
       " 'eval_runtime': 0.078,\n",
       " 'eval_samples_per_second': 128.178,\n",
       " 'eval_steps_per_second': 25.636,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sample = \"This is a sample for inference\"\n",
    "\n",
    "inference_sample_tokenized = tokenizer(\n",
    "    inference_sample,\n",
    "    padding=\"max_length\",\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "inference_result = custom_model(\n",
    "    inference_sample_tokenized[\"input_ids\"].to(\"cuda:0\"), \n",
    "    inference_sample_tokenized[\"attention_mask\"].to(\"cuda:0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0383,  0.0343, -0.1080]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(inference_result.logits)\n",
    "print(inference_result.logits.cpu().argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
