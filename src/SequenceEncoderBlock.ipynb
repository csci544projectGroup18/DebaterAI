{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ROOT_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT_DIR, \"model\", \"pretrained\")\n",
    "assert os.path.isdir(MODEL_DIR)\n",
    "\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = MODEL_DIR\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(PROJECT_ROOT_DIR, \"results\")\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT_DIR, \"logs\")\n",
    "\n",
    "assert os.path.isdir(RESULTS_DIR) and os.path.isdir(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from transformers import MAMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
    "baseGPT2 = GPT2Model.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_NAME = \"mam_adpater\"\n",
    "ADAPTER_CONFIG = MAMConfig()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "SEQUENCE_EMEBDDING_SIZE = 1024\n",
    "CNN_WINDOW_SIZE = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoderBlock(nn.Module):\n",
    "    '''Sequence encoder block\n",
    "    params: \n",
    "        max_sequence_length: Maximum sequence length\n",
    "        adapter_name: Adapter used for fine-tuning pre-trained encoder\n",
    "        adapter_config: Adapter config\n",
    "        cnn_output_channels: Number of output channels of the CNN(=dimension of the sequence embedding)\n",
    "        cnn_window_size: Window size of the CNN\n",
    "    '''\n",
    "    def __init__(\n",
    "            self, \n",
    "            max_sequence_length,\n",
    "            adapter_name,\n",
    "            adapter_config,\n",
    "            cnn_output_channels,\n",
    "            cnn_window_size\n",
    "        ):\n",
    "        super(SequenceEncoderBlock, self).__init__()\n",
    "\n",
    "        #   Pre-trained GPT-2 model\n",
    "        self.gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "        #   Freeze GPT-2 pre-trained parameters\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        #   Add adapter to GPT-2\n",
    "        self.gpt2.add_adapter(adapter_name, config=adapter_config)\n",
    "        self.gpt2.set_active_adapters(adapter_name)\n",
    "\n",
    "        #   CNN layer\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels=self.gpt2.config.hidden_size * 2,\n",
    "            out_channels=cnn_output_channels,\n",
    "            kernel_size=cnn_window_size,\n",
    "            padding=int(cnn_window_size / 2)\n",
    "        )\n",
    "\n",
    "        #   Max pooling layer\n",
    "        self.max_pooling = nn.MaxPool1d(kernel_size=max_sequence_length)\n",
    "\n",
    "        #   Batch normalization layers\n",
    "        self.word_embedding_bn = nn.BatchNorm1d(num_features=self.gpt2.config.hidden_size)\n",
    "        self.encoder_bn = nn.BatchNorm1d(num_features=self.gpt2.config.hidden_size)\n",
    "        self.pooling_bn = nn.BatchNorm1d(cnn_output_channels)\n",
    "\n",
    "    '''Forward propagation\n",
    "    Args:\n",
    "        input_ids: Tensor of shape (B, L) containing the input token IDs\n",
    "        attention_mask: Tensor of shape (B, L) containing the attention mask\n",
    "    '''\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #   Dimension notations:\n",
    "        #   B: batch size\n",
    "        #   L: sequence length\n",
    "        #   H: hidden size\n",
    "        #   C: number of output channels of the CNN (also the dimension of the sequence embedding)\n",
    "\n",
    "        #   Get word embeddings and last hidden states from GPT-2\n",
    "        outputs = self.gpt2(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        word_embeddings = outputs.hidden_states[0]\n",
    "        #   Dimension: (B, L, H)\n",
    "        encoder_hidden_states = outputs.last_hidden_state\n",
    "        #   Dimension: (B, L, H)\n",
    "\n",
    "        #   Batch normalization\n",
    "        bn_word_embeddings = self.word_embedding_bn(\n",
    "            word_embeddings.permute(0, 2, 1)\n",
    "        ).permute(0, 2, 1)\n",
    "        bn_encoder_hidden_states = self.encoder_bn(\n",
    "            encoder_hidden_states.permute(0, 2, 1)\n",
    "        ).permute(0, 2, 1)\n",
    "\n",
    "        #   Concatenate word embeddings and encoder hidden states\n",
    "        concat_embeddings = torch.cat((bn_word_embeddings, bn_encoder_hidden_states), dim=-1)\n",
    "        #   Dimension: (B, L, H * 2)\n",
    "\n",
    "        #   Apply attention mask to the concatenated sequence representation\n",
    "        #   The attetion mask is expanded to dimension (B, L, H * 2), \n",
    "        #   matching the dimension of the concatenated sequence representation.\n",
    "        #   The concatenated sequence representation is multiplied element-wise with the attention mask\n",
    "        #   to zero out the padded positions.\n",
    "        masked_concat_embeddings = concat_embeddings * \\\n",
    "            attention_mask.unsqueeze(-1).expand(concat_embeddings.shape)\n",
    "        \n",
    "        #   Apply CNN layer\n",
    "        cnn_out = self.cnn(masked_concat_embeddings.permute(0, 2, 1))\n",
    "        #   Dimension: (B, C, L)\n",
    "\n",
    "        #   Apply max pooling layer\n",
    "        pooled_output = self.max_pooling(cnn_out)\n",
    "        #   Dimension: (B, C, 1)\n",
    "\n",
    "        #   Apply batch normalization\n",
    "        #   This is the final sequence embedding\n",
    "        sequence_embedding = self.pooling_bn(pooled_output.squeeze(-1))\n",
    "        #   Dimension: (B, C)\n",
    "\n",
    "        return sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyEncoderModel = SequenceEncoderBlock(\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    adapter_name=ADAPTER_NAME,\n",
    "    adapter_config=ADAPTER_CONFIG,\n",
    "    cnn_output_channels=SEQUENCE_EMEBDDING_SIZE,\n",
    "    cnn_window_size=CNN_WINDOW_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sequence = \"This is a sample sequence for testing the sequence encoder block. It contains multiple sentences.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sequence = tokenizer(\n",
    "    sample_sequence,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sample_input_ids = tokenized_sequence[\"input_ids\"].to(DEVICE)\n",
    "sample_attention_mask = tokenized_sequence[\"attention_mask\"].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyEncoderModel.to(DEVICE)\n",
    "MyEncoderModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_result = MyEncoderModel(sample_input_ids, sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5632, 6.7043, 4.0904,  ..., 1.1844, 0.2521, 3.1601]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(encoded_result[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
