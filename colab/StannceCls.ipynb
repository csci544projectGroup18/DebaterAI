{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csci544projectGroup18/DebaterAI/blob/main/colab/StannceCls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jACDlElhywVT"
      },
      "source": [
        "## How to run the code\n",
        "Go to `Runtime > Change runtime type > Hardware accelerator`, select `GPU` and click `save`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmrASX_exEAz",
        "outputId": "e6aa5a97-c4e7-46b8-8019-cb63ccbeb259"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ROOT_DIR = os.getcwd()\n",
        "PRETRAINED_MODEL_DIR = os.path.join(PROJECT_ROOT_DIR, \"models\", \"pretrained\")\n",
        "#assert os.path.isdir(PRETRAINED_MODEL_DIR)\n",
        "\n",
        "#   Path to the directory where the pre-trained model will be saved.\n",
        "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = PRETRAINED_MODEL_DIR\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = PRETRAINED_MODEL_DIR\n",
        "\n",
        "# Colab install the dependencies\n",
        "# %pip install transformers\n",
        "# %pip install adapter-transformers \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SwClgCskxEAz"
      },
      "outputs": [],
      "source": [
        "RESULTS_DIR = os.path.join(PROJECT_ROOT_DIR, \"results\")\n",
        "LOG_DIR = os.path.join(PROJECT_ROOT_DIR, \"logs\")\n",
        "\n",
        "#assert os.path.isdir(RESULTS_DIR) and os.path.isdir(LOG_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hzr_yC4KxEA0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "from transformers import MAMConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9I79OEZxxK3",
        "outputId": "c86b4fc9-a15e-42ce-bcfc-d344b6f52880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "90465f5f67a54e63af9f3c39f83db0db",
            "9544b5a0b74740f19321ce3aaf75876d",
            "0f8acc729c9248dea6723be585915a6d",
            "a7e620d90a014d6db388b784a20ad303",
            "f36aeab95dd9467f878fd3ac205293de",
            "7dfab922cada4d329e16c4b3645570e6",
            "c7666d0fa91e40308e29d5a93586f617",
            "f5a5c17d7b864204ac39f752b0cc17e1",
            "095cc8f742d34e60b7a74e984a315fd8",
            "b90dd09f720248029e3846203272e8cd",
            "939d0171ea144a78bc93b4216ebf14ca",
            "435da7a99846408fa7f133fec4a60541",
            "15ef800d08c243ec84b6bff4d117e803",
            "d553571cd3f64cb89d06f21b773b6a58",
            "6b3fb1ce399b46838d39331c199f35b1",
            "09e828da1e084c059edac0afcc8a5f27",
            "7aeeb03acdec48c194466d9342465168",
            "18959361b4a94673a15dc4440a2c5d6e",
            "8e3ea7d3b8b54f5faf14be75075b7411",
            "223e2023f13a4e77915d3ae12188574a",
            "e21c71905ac64a5db418a2f92ad72f28",
            "eb7048fbae454d51a2191f9c42362905",
            "2465df3d3bcc4e488ea7c43088592a02",
            "5f0ea5084a8f49af91a1ff44589ba93f",
            "a228a74343834f439801707cad43c842",
            "4331526e10714789b83bd48125e40176",
            "06ddbcbd4c4f4d2ea34b990af48b0fab",
            "a5b457ef6008435b9759675a324c0e04",
            "bbf086a2cdfa48a098e1eb4a90573d64",
            "24b33742a0af401e8f5daaae33c59dcd",
            "8f7b2df65c38462880f25f92f5ce4c33",
            "9276db8eaae5480cb2af036121a1a187",
            "b9c6f83d6a304957a126cf989d7563ac"
          ]
        },
        "id": "7rEw0qeBxEA0",
        "outputId": "0095dc7b-a644-4d2c-b577-8a90fdb3d40b"
      },
      "outputs": [],
      "source": [
        "#   Initialize the GPT-2 tokenizer.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
        "\n",
        "#   There is no default padding token in the GPT-2 tokenizer, \n",
        "#   it is set to the end-of-sequence token instead.\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VVrlEJngxEA0"
      },
      "outputs": [],
      "source": [
        "#   Hyperparameters for SequenceEncoder block.\n",
        "ADAPTER_NAME = \"mam_adpater\"\n",
        "ADAPTER_CONFIG = MAMConfig()\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "SEQUENCE_EMEBDDING_SIZE = 1024\n",
        "CNN_WINDOW_SIZE = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eQ2k17K1xEA0"
      },
      "outputs": [],
      "source": [
        "class SequenceEncoderBlock(nn.Module):\n",
        "    '''Sequence encoder block\n",
        "\n",
        "    params: \n",
        "        max_sequence_length: Maximum sequence length\n",
        "        adapter_name: Adapter used for fine-tuning pre-trained encoder\n",
        "        adapter_config: Adapter config\n",
        "        cnn_output_channels: Number of output channels of the CNN(=dimension of the sequence embedding)\n",
        "        cnn_window_size: Window size of the CNN\n",
        "    '''\n",
        "    def __init__(\n",
        "            self, \n",
        "            max_sequence_length,\n",
        "            adapter_name,\n",
        "            adapter_config,\n",
        "            cnn_output_channels,\n",
        "            cnn_window_size\n",
        "        ):\n",
        "        super(SequenceEncoderBlock, self).__init__()\n",
        "\n",
        "        #   Pre-trained GPT-2 model\n",
        "        self.gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
        "\n",
        "        #   Freeze GPT-2 pre-trained parameters\n",
        "        for param in self.gpt2.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        #   Add adapter to GPT-2\n",
        "        self.gpt2.add_adapter(adapter_name, config=adapter_config)\n",
        "        self.gpt2.set_active_adapters(adapter_name)\n",
        "\n",
        "        #   CNN layer\n",
        "        self.cnn = nn.Conv1d(\n",
        "            in_channels=self.gpt2.config.hidden_size * 2,\n",
        "            out_channels=cnn_output_channels,\n",
        "            kernel_size=cnn_window_size,\n",
        "            padding=int(cnn_window_size / 2)\n",
        "        )\n",
        "\n",
        "        #   Max pooling layer\n",
        "        self.max_pooling = nn.MaxPool1d(kernel_size=max_sequence_length)\n",
        "\n",
        "        #   Batch normalization layers\n",
        "        self.word_embedding_bn = nn.BatchNorm1d(num_features=self.gpt2.config.hidden_size)\n",
        "        self.encoder_bn = nn.BatchNorm1d(num_features=self.gpt2.config.hidden_size)\n",
        "        self.pooling_bn = nn.BatchNorm1d(cnn_output_channels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        '''Forward propagation\n",
        "\n",
        "        params:\n",
        "            input_ids: Tensor of shape (B, L) containing the input token IDs\n",
        "            attention_mask: Tensor of shape (B, L) containing the attention mask\n",
        "        '''\n",
        "        #   Dimension notations:\n",
        "        #   B: batch size\n",
        "        #   L: sequence length\n",
        "        #   H: hidden size\n",
        "        #   C: number of output channels of the CNN (also the dimension of the sequence embedding)\n",
        "\n",
        "        #   Get word embeddings and last hidden states from GPT-2\n",
        "        outputs = self.gpt2(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        word_embeddings = outputs.hidden_states[0]\n",
        "        #   Dimension: (B, L, H)\n",
        "        encoder_hidden_states = outputs.last_hidden_state\n",
        "        #   Dimension: (B, L, H)\n",
        "\n",
        "        #   Batch normalization\n",
        "        bn_word_embeddings = self.word_embedding_bn(\n",
        "            word_embeddings.permute(0, 2, 1)\n",
        "        ).permute(0, 2, 1)\n",
        "        bn_encoder_hidden_states = self.encoder_bn(\n",
        "            encoder_hidden_states.permute(0, 2, 1)\n",
        "        ).permute(0, 2, 1)\n",
        "\n",
        "        #   Concatenate word embeddings and encoder hidden states\n",
        "        concat_embeddings = torch.cat((bn_word_embeddings, bn_encoder_hidden_states), dim=-1)\n",
        "        #   Dimension: (B, L, H * 2)\n",
        "\n",
        "        #   Apply attention mask to the concatenated sequence representation\n",
        "        #   The attetion mask is expanded to dimension (B, L, H * 2), \n",
        "        #   matching the dimension of the concatenated sequence representation.\n",
        "        #   The concatenated sequence representation is multiplied element-wise with the attention mask\n",
        "        #   to zero out the padded positions.\n",
        "        masked_concat_embeddings = concat_embeddings * \\\n",
        "            attention_mask.unsqueeze(-1).expand(concat_embeddings.shape)\n",
        "        \n",
        "        #   Apply CNN layer\n",
        "        cnn_out = self.cnn(masked_concat_embeddings.permute(0, 2, 1))\n",
        "        #   Dimension: (B, C, L)\n",
        "\n",
        "        #   Apply max pooling layer\n",
        "        pooled_output = self.max_pooling(cnn_out)\n",
        "        #   Dimension: (B, C, 1)\n",
        "\n",
        "        #   Apply batch normalization\n",
        "        #   This is the final sequence embedding\n",
        "        sequence_embedding = self.pooling_bn(pooled_output.squeeze(-1))\n",
        "        #   Dimension: (B, C)\n",
        "\n",
        "        return sequence_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q4oCLwdsxEA1"
      },
      "outputs": [],
      "source": [
        "#   Hyperparameters for the combined classifier.\n",
        "FF_HIDDEN_SIZE = 4 * SEQUENCE_EMEBDDING_SIZE\n",
        "NUM_CLASSES = 3                                 #   0: Neutral, 1: Agree, 2: Disagree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MlJgzZcdxEA1"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_outputs import SequenceClassifierOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zWw7sIhixEA1"
      },
      "outputs": [],
      "source": [
        "class StanceClassifier(nn.Module):\n",
        "    '''Stance classifier\n",
        "\n",
        "    params:\n",
        "        parent_encoder: Sequence encoder block for parent comments\n",
        "        child_encoder: Sequence encoder block for child comments\n",
        "        context_encoder: Sequence encoder block for comment context\n",
        "        sequence_embedding_size: Dimension of the sequence embedding\n",
        "        ff_hidden_size: Hidden size of the feed-forward layer\n",
        "        num_classes: Number of classes\n",
        "    '''\n",
        "    def __init__(\n",
        "            self, \n",
        "            parent_encoder: SequenceEncoderBlock,\n",
        "            child_encoder: SequenceEncoderBlock,\n",
        "            context_encoder: SequenceEncoderBlock,\n",
        "            loss_fn,\n",
        "            sequence_embedding_size,\n",
        "            ff_hidden_size,\n",
        "            num_classes\n",
        "        ):\n",
        "        super(StanceClassifier, self).__init__()\n",
        "\n",
        "        self.parent_encoder = parent_encoder\n",
        "        self.child_encoder = child_encoder\n",
        "        self.context_encoder = context_encoder\n",
        "\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        #   Feed-forward layer\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(sequence_embedding_size * 2, ff_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_size, sequence_embedding_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(sequence_embedding_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_masks, labels=None):\n",
        "        '''Forward propagation\n",
        "\n",
        "        params:\n",
        "            input_ids: list tensors of shape (B, L) containing the input token IDs\n",
        "            attention_masks: list tensors of shape (B, L) containing the attention masks\n",
        "            labels: Tensor of shape (B,) containing the labels\n",
        "        '''\n",
        "        #   Dimension notations:\n",
        "        #   B: batch size\n",
        "        #   S: dimension of the sequence embedding\n",
        "        #   C: number of classes\n",
        "\n",
        "        parent_embeddings = self.parent_encoder(\n",
        "            input_ids=input_ids[0],\n",
        "            attention_mask=attention_masks[0]\n",
        "        )\n",
        "        child_embeddings = self.child_encoder(\n",
        "            input_ids=input_ids[1],\n",
        "            attention_mask=attention_masks[1]\n",
        "        )\n",
        "        context_embeddings = self.context_encoder(\n",
        "            input_ids=input_ids[2],\n",
        "            attention_mask=attention_masks[2]\n",
        "        )\n",
        "        #   Dimension: 3 * (B, S)\n",
        "\n",
        "        #   Create the combined sequence embedding for classification\n",
        "        combined_embeddings = torch.cat(\n",
        "            (parent_embeddings + context_embeddings, child_embeddings + context_embeddings),\n",
        "            dim=-1\n",
        "        )\n",
        "        #   Dimension: (B, S * 2)\n",
        "\n",
        "        #   Feed-forward layer\n",
        "        logits = self.ff(combined_embeddings)\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oCtHw-t9xEA2"
      },
      "outputs": [],
      "source": [
        "#   Collate function for the combined classifier\n",
        "class CustomDataCollator:\n",
        "    def __init__(self, tokenizer: GPT2Tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        #   Dimension notations:\n",
        "        #   B: batch size\n",
        "        #   L: sequence length\n",
        "\n",
        "        parent_comment = [item['parent_comment'] for item in batch]\n",
        "        child_comment = [item['child_comment'] for item in batch]\n",
        "        context = [item['context'] for item in batch]\n",
        "        labels = [item['label'] for item in batch]\n",
        "\n",
        "        #   Tokenize the input sequences\n",
        "        parent_tokenized = self.tokenizer(\n",
        "            parent_comment,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        child_tokenized = self.tokenizer(\n",
        "            child_comment,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        context_tokenized = self.tokenizer(\n",
        "            context,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        input_ids = [\n",
        "            parent_tokenized['input_ids'], \n",
        "            child_tokenized['input_ids'], \n",
        "            context_tokenized['input_ids']\n",
        "        ]\n",
        "        attention_masks = [\n",
        "            parent_tokenized['attention_mask'],\n",
        "            child_tokenized['attention_mask'],\n",
        "            context_tokenized['attention_mask']\n",
        "        ]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_masks\": attention_masks, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Oh6vz1pNxEA2"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Fj38gtbNxEA2"
      },
      "outputs": [],
      "source": [
        "def custom_compute_metrics(eval_pred: EvalPrediction) -> dict:\n",
        "    '''Compute metrics for the combined classifier\n",
        "\n",
        "    params:\n",
        "        eval_pred: EvalPrediction object\n",
        "    '''\n",
        "    #   Dimension notations:\n",
        "    #   B: batch size\n",
        "    #   C: number of classes\n",
        "\n",
        "    #   Dimension of prediction logits: (B, C)\n",
        "\n",
        "    #   Convert logits to predictions\n",
        "    preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "    #   Dimension: (B,)\n",
        "\n",
        "    #   Compute precision, recall, and F1 score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        eval_pred.label_ids, preds, average=\"weighted\"\n",
        "    )\n",
        "\n",
        "    #   Compute confusion matrix\n",
        "    cm = confusion_matrix(eval_pred.label_ids, preds)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"accuracy\": cm.diagonal() / cm.sum()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WHYfb-NJxEA2"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model: nn.Module, inputs: dict, return_outputs=False):\n",
        "        #   Dimension notations:\n",
        "        #   B: batch size\n",
        "        #   L: sequence length\n",
        "        #   C: number of classes\n",
        "\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        #   Dimension: 3 * (B, L)\n",
        "        attention_masks = inputs[\"attention_masks\"]\n",
        "        #   Dimension: 3 * (B, L)\n",
        "        labels = inputs[\"labels\"]\n",
        "        #   Dimension: (B,)\n",
        "\n",
        "        outputs = model(input_ids, attention_masks, labels=labels)\n",
        "        logits = outputs.logits\n",
        "        loss = outputs.loss\n",
        "\n",
        "        return (loss, logits) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EDkh9M2hxEA2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "import pandas as pd\n",
        "#   Dataset class for the combined classifier\n",
        "\n",
        "class DebaterDataset(Dataset):\n",
        "    def __init__(self, path, is_test):\n",
        "        data = pd.read_csv(path)\n",
        "        self.label_list = data.loc[:, 'label'].tolist()\n",
        "        self.body_parent_list = data.loc[:, 'body_parent'].tolist()\n",
        "        self.body_child_list = data.loc[:, 'body_child'].tolist()\n",
        "        self.submission_text_list = data.loc[:, 'submission_text'].tolist()\n",
        "\n",
        "        L = len(self.label_list)\n",
        "        if not is_test:\n",
        "            self.label_list = self.label_list[:int(L * 0.8)]\n",
        "            self.body_parent_list = self.body_parent_list[:int(L * 0.8)]\n",
        "            self.body_child_list = self.body_child_list[:int(L * 0.8)]\n",
        "            self.submission_text_list = self.submission_text_list[:int(L * 0.8)]\n",
        "        else:\n",
        "            self.label_list = self.label_list[int(L * 0.8):]\n",
        "            self.body_parent_list = self.body_parent_list[int(L * 0.8):]\n",
        "            self.body_child_list = self.body_child_list[int(L * 0.8):]\n",
        "            self.submission_text_list = self.submission_text_list[int(L * 0.8):]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.label_list.__len__()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "          The inidividual item returned by __getitem__ should be a dictionary\n",
        "          containing the following keys:\n",
        "          - parent_comment: string\n",
        "          - child_comment: string\n",
        "          - context: string\n",
        "          - label: int (0: Neutral, 1: Agree, 2: Disagree)\n",
        "        '''\n",
        "        label = self.label_list[index]\n",
        "        parent_comment = self.body_parent_list[index]\n",
        "        child_comment = self.body_child_list[index]\n",
        "        context = self.submission_text_list[index]\n",
        "\n",
        "        return {'parent_comment': parent_comment, \\\n",
        "                'child_comment': child_comment, \\\n",
        "                'context': context, \\\n",
        "                'label': label}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "l-x56InyxEA3"
      },
      "outputs": [],
      "source": [
        "#   Training arguments (need to be changed based on actual performance)\n",
        "TRAINING_EPOCHS = 10\n",
        "BACTH_SIZE = 64\n",
        "LEARNING_RATE = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3e0bb6158e3e4daab8eb280ce57551bd",
            "29641d34a91b4599a9d9bfa75b22252c",
            "d4ae68589e3648adaec6fbc7ddf6c5c5",
            "af105959667148368167de9029e8120d",
            "480ffda3befc47c5a720a801f996bf0a",
            "416f328a7f504bd7a2867ffca620be7c",
            "1e3647b26097459f9b11821c23db7c58",
            "cc5e8eba50a8487794eef831d428ab73",
            "0eeb8a43efee4bc58683c565b2006952",
            "4b8a9d8c6db04fa1943ff548b5cc1a1d",
            "beec2ac6710a4ec98b5938322ab7195b"
          ]
        },
        "id": "ja-DiJBLxEA3",
        "outputId": "496014b1-c94e-412f-b5a3-668670627d53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /lab/xingrui/DebaterAI/colab/models/pretrained/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /lab/xingrui/DebaterAI/colab/models/pretrained/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2Model.\n",
            "\n",
            "All the weights of GPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2Model for predictions without further training.\n",
            "Adding adapter 'mam_adpater'.\n",
            "loading configuration file config.json from cache at /lab/xingrui/DebaterAI/colab/models/pretrained/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /lab/xingrui/DebaterAI/colab/models/pretrained/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2Model.\n",
            "\n",
            "All the weights of GPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2Model for predictions without further training.\n",
            "Adding adapter 'mam_adpater'.\n",
            "loading configuration file config.json from cache at /lab/xingrui/DebaterAI/colab/models/pretrained/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /lab/xingrui/DebaterAI/colab/models/pretrained/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2Model.\n",
            "\n",
            "All the weights of GPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2Model for predictions without further training.\n",
            "Adding adapter 'mam_adpater'.\n"
          ]
        }
      ],
      "source": [
        "SeqEncoder1 = SequenceEncoderBlock(\n",
        "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    adapter_name=ADAPTER_NAME,\n",
        "    adapter_config=ADAPTER_CONFIG,\n",
        "    cnn_output_channels=SEQUENCE_EMEBDDING_SIZE,\n",
        "    cnn_window_size=CNN_WINDOW_SIZE\n",
        ")\n",
        "\n",
        "SeqEncoder2 = SequenceEncoderBlock(\n",
        "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    adapter_name=ADAPTER_NAME,\n",
        "    adapter_config=ADAPTER_CONFIG,\n",
        "    cnn_output_channels=SEQUENCE_EMEBDDING_SIZE,\n",
        "    cnn_window_size=CNN_WINDOW_SIZE\n",
        ")\n",
        "\n",
        "SeqEncoder3 = SequenceEncoderBlock(\n",
        "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    adapter_name=ADAPTER_NAME,\n",
        "    adapter_config=ADAPTER_CONFIG,\n",
        "    cnn_output_channels=SEQUENCE_EMEBDDING_SIZE,\n",
        "    cnn_window_size=CNN_WINDOW_SIZE\n",
        ")\n",
        "\n",
        "CLSModel = StanceClassifier(\n",
        "    parent_encoder=SeqEncoder1,\n",
        "    child_encoder=SeqEncoder2,\n",
        "    context_encoder=SeqEncoder3,\n",
        "    loss_fn=nn.CrossEntropyLoss(),\n",
        "    sequence_embedding_size=SEQUENCE_EMEBDDING_SIZE,\n",
        "    ff_hidden_size=FF_HIDDEN_SIZE,\n",
        "    num_classes=NUM_CLASSES\n",
        ")\n",
        "\n",
        "#   Optimizer and LR scheduler may need to be changed based on actual performance\n",
        "#   This is the default setting from the Trainer implementation\n",
        "optimizer = AdamW(CLSModel.parameters(), lr=LEARNING_RATE)\n",
        "lr_scheduler = LambdaLR(optimizer, lambda epoch: 1 / (epoch + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FAD3WdFVxEA3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "# add dataset\n",
        "train_dataset = DebaterDataset('/lab/xingrui/DebaterAI/data/labeled_data.csv', is_test=False)\n",
        "eval_dataset = DebaterDataset('/lab/xingrui/DebaterAI/data/labeled_data.csv', is_test=True)\n",
        "\n",
        "MyCollator = CustomDataCollator(tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=RESULTS_DIR,\n",
        "    logging_dir=LOG_DIR,\n",
        "    num_train_epochs=TRAINING_EPOCHS,\n",
        "    per_device_train_batch_size=BACTH_SIZE,\n",
        "    per_device_eval_batch_size=BACTH_SIZE,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=CLSModel,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,     #   Change this to the training dataset\n",
        "    eval_dataset=eval_dataset,      #   Change this to the evaluation dataset\n",
        "    data_collator=MyCollator,\n",
        "    optimizers=(optimizer, lr_scheduler),\n",
        "    compute_metrics=custom_compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
            "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
            "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
            "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
            "    environment variable.\n",
            "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
            "***** Running training *****\n",
            "  Num examples = 34315\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2690\n",
            "  Number of trainable parameters = 122558819\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_3178603/2431735171.py\", line 56, in forward\n    child_embeddings = self.child_encoder(\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_3178603/2814622836.py\", line 62, in forward\n    outputs = self.gpt2(\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/adapters/context.py\", line 108, in wrapper_func\n    results = f(self, *args, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 920, in forward\n    outputs = block(\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 454, in forward\n    feed_forward_hidden_states = self.mlp(hidden_states)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 381, in forward\n    hidden_states = self.act(hidden_states)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/activations.py\", line 35, in forward\n    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.70 GiB total capacity; 18.37 GiB already allocated; 44.50 MiB free; 18.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/anaconda3/envs/544/lib/python3.9/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/544/lib/python3.9/site-packages/transformers/trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1794\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1795\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1796\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1797\u001b[0m ):\n\u001b[1;32m   1798\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/anaconda3/envs/544/lib/python3.9/site-packages/transformers/trainer.py:2539\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2538\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2539\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2542\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m labels \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[39m#   Dimension: (B,)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_masks, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m     16\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
            "File \u001b[0;32m~/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
            "File \u001b[0;32m~/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
            "File \u001b[0;32m~/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     90\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/anaconda3/envs/544/lib/python3.9/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_3178603/2431735171.py\", line 56, in forward\n    child_embeddings = self.child_encoder(\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_3178603/2814622836.py\", line 62, in forward\n    outputs = self.gpt2(\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/adapters/context.py\", line 108, in wrapper_func\n    results = f(self, *args, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 920, in forward\n    outputs = block(\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 454, in forward\n    feed_forward_hidden_states = self.mlp(hidden_states)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 381, in forward\n    hidden_states = self.act(hidden_states)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/lab/xingrui/anaconda3/envs/544/lib/python3.9/site-packages/transformers/activations.py\", line 35, in forward\n    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.70 GiB total capacity; 18.37 GiB already allocated; 44.50 MiB free; 18.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06ddbcbd4c4f4d2ea34b990af48b0fab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095cc8f742d34e60b7a74e984a315fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09e828da1e084c059edac0afcc8a5f27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eeb8a43efee4bc58683c565b2006952": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f8acc729c9248dea6723be585915a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5a5c17d7b864204ac39f752b0cc17e1",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_095cc8f742d34e60b7a74e984a315fd8",
            "value": 1042301
          }
        },
        "15ef800d08c243ec84b6bff4d117e803": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aeeb03acdec48c194466d9342465168",
            "placeholder": "​",
            "style": "IPY_MODEL_18959361b4a94673a15dc4440a2c5d6e",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "18959361b4a94673a15dc4440a2c5d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e3647b26097459f9b11821c23db7c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "223e2023f13a4e77915d3ae12188574a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2465df3d3bcc4e488ea7c43088592a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f0ea5084a8f49af91a1ff44589ba93f",
              "IPY_MODEL_a228a74343834f439801707cad43c842",
              "IPY_MODEL_4331526e10714789b83bd48125e40176"
            ],
            "layout": "IPY_MODEL_06ddbcbd4c4f4d2ea34b990af48b0fab"
          }
        },
        "24b33742a0af401e8f5daaae33c59dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29641d34a91b4599a9d9bfa75b22252c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416f328a7f504bd7a2867ffca620be7c",
            "placeholder": "​",
            "style": "IPY_MODEL_1e3647b26097459f9b11821c23db7c58",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "3e0bb6158e3e4daab8eb280ce57551bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29641d34a91b4599a9d9bfa75b22252c",
              "IPY_MODEL_d4ae68589e3648adaec6fbc7ddf6c5c5",
              "IPY_MODEL_af105959667148368167de9029e8120d"
            ],
            "layout": "IPY_MODEL_480ffda3befc47c5a720a801f996bf0a"
          }
        },
        "416f328a7f504bd7a2867ffca620be7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4331526e10714789b83bd48125e40176": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9276db8eaae5480cb2af036121a1a187",
            "placeholder": "​",
            "style": "IPY_MODEL_b9c6f83d6a304957a126cf989d7563ac",
            "value": " 665/665 [00:00&lt;00:00, 24.5kB/s]"
          }
        },
        "435da7a99846408fa7f133fec4a60541": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15ef800d08c243ec84b6bff4d117e803",
              "IPY_MODEL_d553571cd3f64cb89d06f21b773b6a58",
              "IPY_MODEL_6b3fb1ce399b46838d39331c199f35b1"
            ],
            "layout": "IPY_MODEL_09e828da1e084c059edac0afcc8a5f27"
          }
        },
        "480ffda3befc47c5a720a801f996bf0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b8a9d8c6db04fa1943ff548b5cc1a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0ea5084a8f49af91a1ff44589ba93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5b457ef6008435b9759675a324c0e04",
            "placeholder": "​",
            "style": "IPY_MODEL_bbf086a2cdfa48a098e1eb4a90573d64",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "6b3fb1ce399b46838d39331c199f35b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e21c71905ac64a5db418a2f92ad72f28",
            "placeholder": "​",
            "style": "IPY_MODEL_eb7048fbae454d51a2191f9c42362905",
            "value": " 456k/456k [00:00&lt;00:00, 7.27MB/s]"
          }
        },
        "7aeeb03acdec48c194466d9342465168": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dfab922cada4d329e16c4b3645570e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e3ea7d3b8b54f5faf14be75075b7411": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f7b2df65c38462880f25f92f5ce4c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90465f5f67a54e63af9f3c39f83db0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9544b5a0b74740f19321ce3aaf75876d",
              "IPY_MODEL_0f8acc729c9248dea6723be585915a6d",
              "IPY_MODEL_a7e620d90a014d6db388b784a20ad303"
            ],
            "layout": "IPY_MODEL_f36aeab95dd9467f878fd3ac205293de"
          }
        },
        "9276db8eaae5480cb2af036121a1a187": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "939d0171ea144a78bc93b4216ebf14ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9544b5a0b74740f19321ce3aaf75876d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dfab922cada4d329e16c4b3645570e6",
            "placeholder": "​",
            "style": "IPY_MODEL_c7666d0fa91e40308e29d5a93586f617",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "a228a74343834f439801707cad43c842": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b33742a0af401e8f5daaae33c59dcd",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f7b2df65c38462880f25f92f5ce4c33",
            "value": 665
          }
        },
        "a5b457ef6008435b9759675a324c0e04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7e620d90a014d6db388b784a20ad303": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b90dd09f720248029e3846203272e8cd",
            "placeholder": "​",
            "style": "IPY_MODEL_939d0171ea144a78bc93b4216ebf14ca",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 10.4MB/s]"
          }
        },
        "af105959667148368167de9029e8120d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b8a9d8c6db04fa1943ff548b5cc1a1d",
            "placeholder": "​",
            "style": "IPY_MODEL_beec2ac6710a4ec98b5938322ab7195b",
            "value": " 548M/548M [00:04&lt;00:00, 123MB/s]"
          }
        },
        "b90dd09f720248029e3846203272e8cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c6f83d6a304957a126cf989d7563ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbf086a2cdfa48a098e1eb4a90573d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beec2ac6710a4ec98b5938322ab7195b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7666d0fa91e40308e29d5a93586f617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc5e8eba50a8487794eef831d428ab73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ae68589e3648adaec6fbc7ddf6c5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc5e8eba50a8487794eef831d428ab73",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0eeb8a43efee4bc58683c565b2006952",
            "value": 548118077
          }
        },
        "d553571cd3f64cb89d06f21b773b6a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e3ea7d3b8b54f5faf14be75075b7411",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_223e2023f13a4e77915d3ae12188574a",
            "value": 456318
          }
        },
        "e21c71905ac64a5db418a2f92ad72f28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7048fbae454d51a2191f9c42362905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f36aeab95dd9467f878fd3ac205293de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a5c17d7b864204ac39f752b0cc17e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
